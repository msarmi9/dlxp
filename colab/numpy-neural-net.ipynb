{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Numpy Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:19.437060Z",
     "start_time": "2021-03-05T03:48:17.691959Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -qU numpy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.264250Z",
     "start_time": "2021-03-05T03:48:19.443183Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.269571Z",
     "start_time": "2021-03-05T03:48:20.267375Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "Our goal is to write a simple two layer neural network in numpy, starting from a single layer logistic network. First, we'll define our loss function, activations, linear layer, and SGD. Then we'll tie it all together in a single Sequential model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.276179Z",
     "start_time": "2021-03-05T03:48:20.271846Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    \"\"\"Container for the forward and backward pass of BCE.\"\"\"\n",
    "    \n",
    "    def __call__(self, y_hat, y):\n",
    "        return self.forward(y_hat, y)\n",
    "    \n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"Return binary cross entropy given predictions and targets.\"\"\"\n",
    "        self.y_hat, self.y = y_hat.clip(min=1e-8, max=1-1e-8), y\n",
    "        return -np.where(y==1, np.log(self.y_hat), np.log(1 - self.y_hat))\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate the gradient with respect to predictions.\"\"\"\n",
    "        return (self.y_hat - self.y) / (self.y_hat * (1 - self.y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.282343Z",
     "start_time": "2021-03-05T03:48:20.278462Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Container for the forward and backward pass of sigmoid.\"\"\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a sigmoid layer.\"\"\"\n",
    "        self.y_hat = np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return self.y_hat\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient given the preceding gradient.\"\"\"\n",
    "        return self.y_hat * (1 - self.y_hat) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.289204Z",
     "start_time": "2021-03-05T03:48:20.284337Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Container for the forward and backward pass of a linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inp, n_out):\n",
    "        \"\"\"Initialise layer with random weights and zero bias.\"\"\"\n",
    "        k = 1 / np.sqrt(n_inp)\n",
    "        self.weights = np.random.uniform(-k, k, (n_inp, n_out))\n",
    "        self.bias = np.zeros(n_out)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a linear layer.\"\"\"\n",
    "        self.x = x\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient given the preceding gradient.\"\"\"\n",
    "        self.grad_w = (self.x[:,:,None] @ grad[:,None,:]).mean(axis=0)\n",
    "        self.grad_b = grad.mean(axis=0)\n",
    "        return grad @ self.weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "It's finally time to string together all of the work we've done so far into a complete network. Then we'll put it to the test on the [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) and see how we compare to sklearn's built-in logistic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.295611Z",
     "start_time": "2021-03-05T03:48:20.291243Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"Container for a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, criterion):\n",
    "        \"\"\"Initialise layers and loss criterion.\"\"\"\n",
    "        self.layers = layers\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through the network.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate gradients to the start of the network.\"\"\"\n",
    "        grad = self.criterion.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.303321Z",
     "start_time": "2021-03-05T03:48:20.299170Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Container for updating a model's weights via SGD.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr):\n",
    "        \"\"\"Initialise model parameters and learning rate.\"\"\"\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "                  \n",
    "    def step(self):\n",
    "        \"\"\"Update weights and biases of all linear layers.\"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.weights -= self.lr * layer.grad_w\n",
    "                layer.bias -= self.lr * layer.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Evaluation Metric\n",
    "\n",
    "For simplicity, we'll just consider accuracy as our evaluation metric for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.309781Z",
     "start_time": "2021-03-05T03:48:20.307382Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute accuracy given soft binary predictions.\"\"\"\n",
    "    y_pred = y_hat > 0.5\n",
    "    return (y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "To make life easier, let's wrap all of the functionality we'll need to train a network in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.318828Z",
     "start_time": "2021-03-05T03:48:20.311572Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Container for training a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_dl, val_dl, metric):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.metric = metric\n",
    "        \n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"Train for one epoch and return the loss.\"\"\"\n",
    "        loss, n = 0, 0\n",
    "        for x, y in self.train_dl:\n",
    "            y_hat = self.model(x)\n",
    "            batch_loss = self.model.criterion(y_hat, y).sum()\n",
    "            self.model.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n\n",
    "            \n",
    "    def train(self, n_epochs, log_level=1):\n",
    "        \"\"\"Train for several epochs.\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self.train_one_epoch()\n",
    "            val_loss, val_metric = self.evaluate(self.val_dl)\n",
    "            if (epoch + 1) % log_level == 0:\n",
    "                print(f\"epoch= {epoch:2d} | loss= {loss:.3f} | \"\n",
    "                      f\"val_loss= {val_loss:.3f} | val_metric= {val_metric:.3f}\")\n",
    "    \n",
    "    def evaluate(self, dl):\n",
    "        \"\"\"Return loss and metric on validation or test set.\"\"\"\n",
    "        loss, n, metric = 0, 0, 0\n",
    "        for x, y in dl:\n",
    "            y_hat = self.model(x)\n",
    "            batch_loss = self.model.criterion(y_hat, y).sum()\n",
    "            batch_metric = self.metric(y_hat, y)\n",
    "            metric += len(y) * batch_metric\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n, metric / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data\n",
    "\n",
    "We'll use sklearn's [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) for our binary classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.339292Z",
     "start_time": "2021-03-05T03:48:20.321423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Train-test-split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.345298Z",
     "start_time": "2021-03-05T03:48:20.341892Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(X_train, X_val):\n",
    "    \"\"\"Normalize training and validation data using training stats.\"\"\"\n",
    "    for j in range(X_train.shape[1]):\n",
    "        mu, sigma = X_train[:,j].mean(), X_train[:,j].std()\n",
    "        X_train[:,j] = (X_train[:,j] - mu) / sigma\n",
    "        X_val[:,j] = (X_val[:,j] - mu) / sigma\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.351311Z",
     "start_time": "2021-03-05T03:48:20.347188Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize with training stats\n",
    "X_train, X_val = normalize(X_train, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train in batches, we'll need to implement our own version of pytorch's datasets and dataloaders, since we're doing everything in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.356668Z",
     "start_time": "2021-03-05T03:48:20.352993Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Container for returning inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"Initialise inputs and re-shape targets as a column vector.\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __setitem__(self, idx, val):\n",
    "        self.X[idx], self.y[idx] = val\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.363438Z",
     "start_time": "2021-03-05T03:48:20.358735Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Container for returning a mini-batch of inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, ds, batch_size, shuffle=False):\n",
    "        \"\"\"Initialise dataset and batch size.\"\"\"\n",
    "        self.ds = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a mini-batch of inputs and targets.\"\"\"\n",
    "        if self.shuffle: self.shuffle_data()\n",
    "        n_batches = len(self.ds) // self.batch_size\n",
    "        for i in range(n_batches):\n",
    "            yield self.ds[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            \n",
    "    def shuffle_data(self):\n",
    "        \"\"\"Shuffle inputs and targets.\"\"\"\n",
    "        idxs = np.random.permutation(len(self.ds))\n",
    "        self.ds = Dataset(*self.ds[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.369191Z",
     "start_time": "2021-03-05T03:48:20.365561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_ds = Dataset(X_train, y_train)\n",
    "val_ds = Dataset(X_val, y_val)\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(X_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Now we're ready to put our model to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.383105Z",
     "start_time": "2021-03-05T03:48:20.370758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.421 | val_loss= 0.263 | val_metric= 0.939\n",
      "epoch=  1 | loss= 0.246 | val_loss= 0.198 | val_metric= 0.956\n",
      "epoch=  2 | loss= 0.199 | val_loss= 0.169 | val_metric= 0.965\n",
      "epoch=  3 | loss= 0.170 | val_loss= 0.152 | val_metric= 0.965\n",
      "epoch=  4 | loss= 0.158 | val_loss= 0.140 | val_metric= 0.965\n"
     ]
    }
   ],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.1)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl, metric)\n",
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with sklearn\n",
    "\n",
    "Let's see how our logistic network stacks up against sklearn's logistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.400199Z",
     "start_time": "2021-03-05T03:48:20.385741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're close!\n",
    "sklearn_model = LogisticRegression(random_state=seed)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer Network\n",
    "\n",
    "Now let's see if we can do a bit better by training a deeper network. First, we'll need to implement $\\text{ReLU}$ for the activations in between our linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.406022Z",
     "start_time": "2021-03-05T03:48:20.402271Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"Container for the forward and backward pass of ReLU.\"\"\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through ReLU.\"\"\"\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Return the gradient where x is positive, otherwise zero.\"\"\"\n",
    "        return np.where(self.x > 0, grad, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:48:20.445734Z",
     "start_time": "2021-03-05T03:48:20.407988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.567 | val_loss= 0.485 | val_metric= 0.921\n",
      "epoch=  1 | loss= 0.443 | val_loss= 0.361 | val_metric= 0.921\n",
      "epoch=  2 | loss= 0.338 | val_loss= 0.273 | val_metric= 0.930\n",
      "epoch=  3 | loss= 0.268 | val_loss= 0.220 | val_metric= 0.939\n",
      "epoch=  4 | loss= 0.225 | val_loss= 0.187 | val_metric= 0.939\n",
      "epoch=  5 | loss= 0.192 | val_loss= 0.165 | val_metric= 0.939\n",
      "epoch=  6 | loss= 0.172 | val_loss= 0.150 | val_metric= 0.939\n",
      "epoch=  7 | loss= 0.156 | val_loss= 0.138 | val_metric= 0.939\n",
      "epoch=  8 | loss= 0.144 | val_loss= 0.130 | val_metric= 0.947\n",
      "epoch=  9 | loss= 0.135 | val_loss= 0.123 | val_metric= 0.956\n"
     ]
    }
   ],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 20), ReLU(), Linear(20, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.10)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl, metric)\n",
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for today. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
