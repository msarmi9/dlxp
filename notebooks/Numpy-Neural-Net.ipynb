{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Numpy Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:34.831879Z",
     "start_time": "2021-02-04T05:28:33.016985Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -qU numpy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.623973Z",
     "start_time": "2021-02-04T05:28:34.835202Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.629525Z",
     "start_time": "2021-02-04T05:28:35.627257Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "Our goal is to write a simple two layer neural network in numpy, starting from a single layer logistic network. First, we'll define our loss function, activations, linear layer, and SGD. Then we'll tie it all together in a single Sequential model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.635977Z",
     "start_time": "2021-02-04T05:28:35.631681Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    \"\"\"Container for the forward and backward pass of BCE.\"\"\"\n",
    "    \n",
    "    def __call__(self, y_hat, y):\n",
    "        return self.forward(y_hat, y)\n",
    "    \n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"Return binary cross entropy given predictions and targets.\"\"\"\n",
    "        self.y_hat, self.y = y_hat.clip(min=1e-8, max=1-1e-8), y\n",
    "        return -np.where(y==1, np.log(self.y_hat), np.log(1 - self.y_hat))\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate the gradient with respect to predictions.\"\"\"\n",
    "        return (self.y_hat - self.y) / (self.y_hat * (1 - self.y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.641567Z",
     "start_time": "2021-02-04T05:28:35.637428Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Container for the forward and backward pass of sigmoid.\"\"\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a sigmoid layer.\"\"\"\n",
    "        self.y_hat = np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return self.y_hat\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient given the preceding gradient.\"\"\"\n",
    "        return self.y_hat * (1 - self.y_hat) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.647799Z",
     "start_time": "2021-02-04T05:28:35.643434Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Container for the forward and backward pass of a linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inp, n_out):\n",
    "        \"\"\"Initialise layer with random weights and zero bias.\"\"\"\n",
    "        k = 1 / np.sqrt(n_inp)\n",
    "        self.weights = np.random.uniform(-k, k, (n_inp, n_out))\n",
    "        self.bias = np.zeros(n_out)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a linear layer.\"\"\"\n",
    "        self.x = x\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient given the preceding gradient.\"\"\"\n",
    "        self.grad_w = (self.x[:,:,None] @ grad[:,None,:]).mean(axis=0)\n",
    "        self.grad_b = grad.mean(axis=0)\n",
    "        return grad @ self.weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "It's finally time to string together all of the work we've done so far into a complete network. Then we'll put it to the test on the [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) and see how we compare to sklearn's built-in logistic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.653865Z",
     "start_time": "2021-02-04T05:28:35.649362Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"Container for a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, criterion):\n",
    "        \"\"\"Initialise layers and loss criterion.\"\"\"\n",
    "        self.layers = layers\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through the network.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate gradients to the start of the network.\"\"\"\n",
    "        grad = self.criterion.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.662077Z",
     "start_time": "2021-02-04T05:28:35.658389Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Container for updating a model's weights via SGD.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr):\n",
    "        \"\"\"Initialise model parameters and learning rate.\"\"\"\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "                  \n",
    "    def step(self):\n",
    "        \"\"\"Update weights and biases of all linear layers.\"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.weights -= self.lr * layer.grad_w\n",
    "                layer.bias -= self.lr * layer.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Evaluation Metric\n",
    "\n",
    "For simplicity, we'll just consider accuracy as our evaluation metric for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.668231Z",
     "start_time": "2021-02-04T05:28:35.665403Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute accuracy given soft binary predictions.\"\"\"\n",
    "    y_pred = y_hat > 0.5\n",
    "    return (y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "To make life easier, let's wrap all of the functionality we'll need to train a network in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.677382Z",
     "start_time": "2021-02-04T05:28:35.670462Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Container for training a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_dl, val_dl, metric):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.metric = metric\n",
    "        \n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"Train for one epoch and return the loss.\"\"\"\n",
    "        loss, n = 0, 0\n",
    "        for x, y in self.train_dl:\n",
    "            y_hat = self.model(x)\n",
    "            batch_loss = self.model.criterion(y_hat, y).sum()\n",
    "            self.model.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n\n",
    "            \n",
    "    def train(self, n_epochs, log_level=1):\n",
    "        \"\"\"Train for several epochs.\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self.train_one_epoch()\n",
    "            val_loss, val_metric = self.evaluate(self.val_dl)\n",
    "            if (epoch + 1) % log_level == 0:\n",
    "                print(f\"{epoch= :2d} | {loss= :.3f} | {val_loss= :.3f} | {val_metric= :.3f}\")\n",
    "    \n",
    "    def evaluate(self, dl):\n",
    "        \"\"\"Return loss and metric on validation or test set.\"\"\"\n",
    "        loss, n, metric = 0, 0, 0\n",
    "        for x, y in dl:\n",
    "            y_hat = self.model(x)\n",
    "            batch_loss = self.model.criterion(y_hat, y).sum()\n",
    "            batch_metric = self.metric(y_hat, y)\n",
    "            metric += len(y) * batch_metric\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n, metric / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data\n",
    "\n",
    "We'll use sklearn's [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) for our binary classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.695976Z",
     "start_time": "2021-02-04T05:28:35.679020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.702158Z",
     "start_time": "2021-02-04T05:28:35.697606Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test-split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.706731Z",
     "start_time": "2021-02-04T05:28:35.703528Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(X_train, X_val):\n",
    "    \"\"\"Normalize training and validation data using training stats.\"\"\"\n",
    "    for j in range(X_train.shape[1]):\n",
    "        mu, sigma = X_train[:,j].mean(), X_train[:,j].std()\n",
    "        X_train[:,j] = (X_train[:,j] - mu) / sigma\n",
    "        X_val[:,j] = (X_val[:,j] - mu) / sigma\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.713263Z",
     "start_time": "2021-02-04T05:28:35.709100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize with training stats\n",
    "X_train, X_val = normalize(X_train, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train in batches, we'll need to implement our own version of pytorch's datasets and dataloaders, since we're doing everything in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.718973Z",
     "start_time": "2021-02-04T05:28:35.714702Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Container for returning inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"Initialise inputs and re-shape targets as a column vector.\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __setitem__(self, idx, val):\n",
    "        self.X[idx], self.y[idx] = val\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.724497Z",
     "start_time": "2021-02-04T05:28:35.720415Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Container for returning a mini-batch of inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, ds, batch_size, shuffle=False):\n",
    "        \"\"\"Initialise dataset and batch size.\"\"\"\n",
    "        self.ds = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a mini-batch of inputs and targets.\"\"\"\n",
    "        if self.shuffle: self.shuffle_data()\n",
    "        n_batches = len(self.ds) // self.batch_size\n",
    "        for i in range(n_batches):\n",
    "            yield self.ds[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            \n",
    "    def shuffle_data(self):\n",
    "        \"\"\"Shuffle inputs and targets.\"\"\"\n",
    "        idxs = np.random.permutation(len(self.ds))\n",
    "        self.ds = Dataset(*self.ds[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.729295Z",
     "start_time": "2021-02-04T05:28:35.726374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_ds = Dataset(X_train, y_train)\n",
    "val_ds = Dataset(X_val, y_val)\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(X_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Now we're ready to put our model to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.735837Z",
     "start_time": "2021-02-04T05:28:35.730826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.1)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.756740Z",
     "start_time": "2021-02-04T05:28:35.738384Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.421 | val_loss= 0.263 | val_metric= 0.939\n",
      "epoch=  1 | loss= 0.246 | val_loss= 0.198 | val_metric= 0.956\n",
      "epoch=  2 | loss= 0.199 | val_loss= 0.169 | val_metric= 0.965\n",
      "epoch=  3 | loss= 0.170 | val_loss= 0.152 | val_metric= 0.965\n",
      "epoch=  4 | loss= 0.158 | val_loss= 0.140 | val_metric= 0.965\n",
      "epoch=  5 | loss= 0.144 | val_loss= 0.132 | val_metric= 0.965\n",
      "epoch=  6 | loss= 0.138 | val_loss= 0.125 | val_metric= 0.965\n",
      "epoch=  7 | loss= 0.131 | val_loss= 0.120 | val_metric= 0.965\n",
      "epoch=  8 | loss= 0.126 | val_loss= 0.116 | val_metric= 0.965\n",
      "epoch=  9 | loss= 0.120 | val_loss= 0.112 | val_metric= 0.965\n"
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with sklearn\n",
    "\n",
    "Let's see how our logistic network stacks up against sklearn's logistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.772816Z",
     "start_time": "2021-02-04T05:28:35.758341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're close!\n",
    "sklearn_model = LogisticRegression(random_state=seed)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer Network\n",
    "\n",
    "Now let's see if we can do a bit better by training a deeper network. First, we'll need to implement $\\text{ReLU}$ for the activations in between our linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.778857Z",
     "start_time": "2021-02-04T05:28:35.774876Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"Container for the forward and backward pass of ReLU.\"\"\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through ReLU.\"\"\"\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Return the gradient where x is positive, otherwise zero.\"\"\"\n",
    "        return np.where(self.x > 0, grad, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.784982Z",
     "start_time": "2021-02-04T05:28:35.781036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 20), ReLU(), Linear(20, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.10)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-04T05:28:35.826758Z",
     "start_time": "2021-02-04T05:28:35.791279Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.617 | val_loss= 0.496 | val_metric= 0.930\n",
      "epoch=  1 | loss= 0.457 | val_loss= 0.352 | val_metric= 0.939\n",
      "epoch=  2 | loss= 0.340 | val_loss= 0.258 | val_metric= 0.939\n",
      "epoch=  3 | loss= 0.258 | val_loss= 0.204 | val_metric= 0.939\n",
      "epoch=  4 | loss= 0.210 | val_loss= 0.173 | val_metric= 0.939\n",
      "epoch=  5 | loss= 0.183 | val_loss= 0.153 | val_metric= 0.939\n",
      "epoch=  6 | loss= 0.162 | val_loss= 0.139 | val_metric= 0.939\n",
      "epoch=  7 | loss= 0.141 | val_loss= 0.128 | val_metric= 0.947\n",
      "epoch=  8 | loss= 0.133 | val_loss= 0.120 | val_metric= 0.965\n",
      "epoch=  9 | loss= 0.122 | val_loss= 0.114 | val_metric= 0.974\n"
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for today. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
